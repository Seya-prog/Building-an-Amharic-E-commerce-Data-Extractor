{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a443e3c",
   "metadata": {},
   "source": [
    "# Task 3 – Fine-tune a Transformer NER model for EthioMart\n",
    "Trains a token-classification model (default **XLM-Roberta-base**) on your labelled Amharic e-commerce data.\n",
    "\n",
    "*Input*: `data/ner/ner_labeled.conll` (100+ annotated sentences).\n",
    "*Output*: fine-tuned model + metrics in `models/ner-xlmr/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b62f06",
   "metadata": {},
   "source": [
    "## 0  Setup\n",
    "Run the install cell once, preferably in Colab with GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bae58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # !pip install -q transformers datasets evaluate seqeval accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec614681",
   "metadata": {},
   "source": [
    "## 1  Hyper-parameters (edit here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed188b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hyper-parameters (edit here)\n",
    "model_ckpt = 'xlm-roberta-base'   # or 'afroxlmr-base', 'bert-tiny-amharic'\n",
    "learning_rate = 5e-5              # try 1e-5 – 5e-5\n",
    "epochs = 22\n",
    "batch_size = 8\n",
    "max_length = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb233d",
   "metadata": {},
   "source": [
    "## 2  Load and parse CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and parse CoNLL\n",
    "from pathlib import Path; import re, random\n",
    "import datasets, evaluate, torch\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                         TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "\n",
    "DATA_PATH = Path('ner_labeled.conll')\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f'{DATA_PATH} not found – export your labels there.')\n",
    "\n",
    "def read_conll(path):\n",
    "    sents, labels = [], []\n",
    "    cur_toks, cur_tags = [], []\n",
    "    for line in path.read_text(encoding='utf-8').splitlines():\n",
    "        if not line.strip():\n",
    "            if cur_toks:\n",
    "                sents.append(cur_toks); labels.append(cur_tags)\n",
    "                cur_toks, cur_tags = [], []\n",
    "            continue\n",
    "        parts = re.split('[\\t ]+', line.strip())\n",
    "        tok, tag = parts[0], parts[1] if len(parts) > 1 else 'O'\n",
    "        cur_toks.append(tok); cur_tags.append(tag)\n",
    "    if cur_toks: sents.append(cur_toks); labels.append(cur_tags)\n",
    "    return list(zip(sents, labels))\n",
    "\n",
    "examples = read_conll(DATA_PATH)\n",
    "print('Sentences:', len(examples))\n",
    "label_list = sorted({t for _x, tags in examples for t in tags})\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdc666",
   "metadata": {},
   "source": [
    "## 3  Dataset & tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset & tokenisation\n",
    "def to_hf(example):\n",
    "    toks, tags = example\n",
    "    return {'tokens': toks, 'ner_tags': [label2id[t] for t in tags]}\n",
    "\n",
    "ds = datasets.Dataset.from_list([to_hf(e) for e in examples])\n",
    "ds = ds.shuffle(seed=42).train_test_split(test_size=0.2)\n",
    "train_ds, eval_ds = ds['train'], ds['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(example):\n",
    "    enc = tokenizer(example['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)\n",
    "    \n",
    "    word_ids = enc.word_ids()\n",
    "    label_ids = []\n",
    "    prev = None\n",
    "    \n",
    "    for wid in word_ids:\n",
    "        if wid is None:\n",
    "            label_ids.append(-100)\n",
    "        elif wid != prev:\n",
    "            label_ids.append(example['ner_tags'][wid])\n",
    "            prev = wid\n",
    "        else:\n",
    "            label_ids.append(example['ner_tags'][wid])\n",
    "            \n",
    "    enc['labels'] = label_ids\n",
    "    return enc\n",
    "\n",
    "train_ds = train_ds.map(tokenize, remove_columns=train_ds.column_names)\n",
    "eval_ds = eval_ds.map(tokenize, remove_columns=eval_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa055d",
   "metadata": {},
   "source": [
    "## 4  Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fine-tune\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='models/ner-xlmr',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_overall_f1',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def metrics(p):\n",
    "    logits, labels = p\n",
    "    preds = logits.argmax(-1)\n",
    "    true_preds, true_labels = [], []\n",
    "    \n",
    "    for pred, lab in zip(preds, labels):\n",
    "        tp, tl = [], []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                tp.append(id2label[p_i])\n",
    "                tl.append(id2label[l_i])\n",
    "        true_preds.append(tp)\n",
    "        true_labels.append(tl)\n",
    "    \n",
    "    return seqeval.compute(\n",
    "        predictions=true_preds, \n",
    "        references=true_labels,\n",
    "        zero_division=0  # Suppress warnings\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=metrics\n",
    ")\n",
    "\n",
    "# Uncomment to train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ab4dd",
   "metadata": {},
   "source": [
    "After training, the best checkpoint resides in `models/ner-xlmr/`. Load with:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('models/ner-xlmr')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
