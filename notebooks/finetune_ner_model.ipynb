{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Avq1GoCOHka"
      },
      "source": [
        "# Task 3 – Fine-tune a Transformer NER model for EthioMart\n",
        "Trains a token-classification model (default **XLM-Roberta-base**) on your labelled Amharic e-commerce data.\n",
        "\n",
        "*Input*: `data/ner/ner_labeled.conll` (100+ annotated sentences).\n",
        "*Output*: fine-tuned model + metrics in `models/ner-xlmr/`."
      ],
      "id": "1Avq1GoCOHka"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGU6JM6mOHkc"
      },
      "source": [
        "## 0  Setup\n",
        "Run the install cell once, preferably in Colab with GPU runtime."
      ],
      "id": "DGU6JM6mOHkc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dL9KSnk-OHkd",
        "outputId": "2cb27bc0-36b6-485f-d06a-e580108ee1cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets evaluate seqeval accelerate"
      ],
      "id": "dL9KSnk-OHkd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCyL3JszOHke"
      },
      "source": [
        "## 1  Hyper-parameters (edit here)"
      ],
      "id": "GCyL3JszOHke"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bUabRDa5OHke"
      },
      "outputs": [],
      "source": [
        "model_ckpt = 'xlm-roberta-base'   # or 'afroxlmr-base', 'bert-tiny-amharic'\n",
        "learning_rate = 2e-5              # try 1e-5 – 5e-5\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "max_length = 128\n"
      ],
      "id": "bUabRDa5OHke"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTyxnYWOHke"
      },
      "source": [
        "## 2  Load and parse CoNLL"
      ],
      "id": "piTyxnYWOHke"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MaDVZzDcOHke",
        "outputId": "01d89b89-a177-4afa-eeea-00dc0ac2881f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "data/ner/ner_labeled.conll not found – export your labels there.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3445196429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/ner/ner_labeled.conll'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_PATH} not found – export your labels there.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: data/ner/ner_labeled.conll not found – export your labels there."
          ]
        }
      ],
      "source": [
        "from pathlib import Path; import re, random\n",
        "import datasets, evaluate, torch\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
        "    \tTrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
        "\n",
        "DATA_PATH = Path('../data/ner/ner_labeled.conll')\n",
        "if not DATA_PATH.exists():\n",
        "    \traise FileNotFoundError(f'{DATA_PATH} not found – export your labels there.')\n",
        "\n",
        "def read_conll(path):\n",
        "    \tsents, labels = [], []\n",
        "    \tcur_toks, cur_tags = [], []\n",
        "    \tfor line in path.read_text(encoding='utf-8').splitlines():\n",
        "    \t\tif not line.strip():\n",
        "    \t\t\tif cur_toks:\n",
        "    \t\t\t\tsents.append(cur_toks); labels.append(cur_tags)\n",
        "    \t\t\t\tcur_toks, cur_tags = [], []\n",
        "    \t\t\tcontinue\n",
        "    \t\tparts = re.split('[\t ]+', line.strip())\n",
        "    \t\ttok, tag = parts[0], parts[1] if len(parts) > 1 else 'O'\n",
        "    \t\tcur_toks.append(tok); cur_tags.append(tag)\n",
        "    \tif cur_toks: sents.append(cur_toks); labels.append(cur_tags)\n",
        "    \treturn list(zip(sents, labels))\n",
        "examples = read_conll(DATA_PATH)\n",
        "print('Sentences:', len(examples))\n",
        "label_list = sorted({t for _x, tags in examples for t in tags})\n",
        "label2id = {l: i for i, l in enumerate(label_list)}; id2label = {i: l for l, i in label2id.items()}\n"
      ],
      "id": "MaDVZzDcOHke"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbRsKUg2OHkf"
      },
      "source": [
        "## 3  Dataset & tokenisation"
      ],
      "id": "LbRsKUg2OHkf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL5azba7OHkf"
      },
      "outputs": [],
      "source": [
        "\tdef to_hf(example):\n",
        "\t\t\t\ttoks, tags = example\n",
        "\t\t\t\treturn {'tokens': toks, 'ner_tags': [label2id[t] for t in tags]}\n",
        "\tds = datasets.Dataset.from_list([to_hf(e) for e in examples])\n",
        "\tds = ds.shuffle(seed=42).train_test_split(test_size=0.2)\n",
        "\ttrain_ds, eval_ds = ds['train'], ds['test']\n",
        "\ttokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "\tdef tokenize(batch):\n",
        "\t\t\t\tenc = tokenizer(batch['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)\n",
        "\t\t\t\tlabels = []\n",
        "\t\t\t\tfor i, word_ids in enumerate(enc.word_ids(batch_index=None)):\n",
        "\t\t\t\t\tword_ids = enc.word_ids(batch_index=i)\n",
        "\t\t\t\t\tlabel_ids, prev = [], None\n",
        "\t\t\t\t\tfor wid in word_ids:\n",
        "\t\t\t\t\t\tif wid is None: label_ids.append(-100)\n",
        "\t\t\t\t\t\telif wid != prev:\n",
        "\t\t\t\t\t\t\tlabel_ids.append(batch['ner_tags'][i][wid]); prev = wid\n",
        "\t\t\t\t\t\telse: label_ids.append(batch['ner_tags'][i][wid])\n",
        "\t\t\t\t\tlabels.append(label_ids)\n",
        "\t\t\t\tenc['labels'] = labels\n",
        "\t\t\t\treturn enc\n",
        "\ttrain_ds = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
        "\teval_ds = eval_ds.map(tokenize, batched=True, remove_columns=eval_ds.column_names)\n"
      ],
      "id": "oL5azba7OHkf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSlT3lfGOHkf"
      },
      "source": [
        "## 4  Fine-tune"
      ],
      "id": "OSlT3lfGOHkf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8ixQY78OHkg"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
        "args = TrainingArguments(\n",
        "        output_dir='models/ner-xlmr',\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "seqeval = evaluate.load('seqeval')\n",
        "def metrics(p):\n",
        "        logits, labels = p\n",
        "        preds = logits.argmax(-1)\n",
        "        true_preds, true_labels = [], []\n",
        "        for pred, lab in zip(preds, labels):\n",
        "            tp, tl = [], []\n",
        "            for p_i, l_i in zip(pred, lab):\n",
        "                if l_i != -100:\n",
        "                    tp.append(id2label[p_i]); tl.append(id2label[l_i])\n",
        "            true_preds.append(tp); true_labels.append(tl)\n",
        "        return seqeval.compute(predictions=true_preds, references=true_labels)\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=data_collator, compute_metrics=metrics)\n",
        "# Uncomment to train\n",
        "    # trainer.train()\n"
      ],
      "id": "r8ixQY78OHkg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7QwqByPOHkg"
      },
      "source": [
        "After training, the best checkpoint resides in `models/ner-xlmr/`. Load with:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForTokenClassification\n",
        "model = AutoModelForTokenClassification.from_pretrained('models/ner-xlmr')\n",
        "```"
      ],
      "id": "P7QwqByPOHkg"
    }
  ]
}