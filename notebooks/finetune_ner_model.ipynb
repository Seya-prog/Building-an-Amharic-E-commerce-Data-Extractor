{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 – Fine-tune a Transformer NER model for EthioMart\n",
    "Trains a token-classification model (default **XLM-Roberta-base**) on your labelled Amharic e-commerce data.\n",
    "\n",
    "*Input*: `data/ner/ner_labeled.conll` (100+ annotated sentences).\n",
    "*Output*: fine-tuned model + metrics in `models/ner-xlmr/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0  Setup\n",
    "Run the install cell once, preferably in Colab with GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets evaluate seqeval accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1  Hyper-parameters (edit here)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'xlm-roberta-base'   # or 'afroxlmr-base', 'bert-tiny-amharic'\n",
    "learning_rate = 2e-5              # try 1e-5 – 5e-5\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "max_length = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2  Load and parse CoNLL"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path; import re, random\n",
    "import datasets, evaluate, torch\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n    \tTrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "\n",
    "DATA_PATH = Path('data/ner/ner_labeled.conll')\n",
    "if not DATA_PATH.exists():\n    \traise FileNotFoundError(f'{DATA_PATH} not found – export your labels there.')\n",
    "\n",
    "def read_conll(path):\n    \tsents, labels = [], []\n    \tcur_toks, cur_tags = [], []\n    \tfor line in path.read_text(encoding='utf-8').splitlines():\n    \t\tif not line.strip():\n    \t\t\tif cur_toks:\n    \t\t\t\tsents.append(cur_toks); labels.append(cur_tags)\n    \t\t\t\tcur_toks, cur_tags = [], []\n    \t\t\tcontinue\n    \t\tparts = re.split('[\t ]+', line.strip())\n    \t\ttok, tag = parts[0], parts[1] if len(parts) > 1 else 'O'\n    \t\tcur_toks.append(tok); cur_tags.append(tag)\n    \tif cur_toks: sents.append(cur_toks); labels.append(cur_tags)\n    \treturn list(zip(sents, labels))\n",
    "examples = read_conll(DATA_PATH)\n",
    "print('Sentences:', len(examples))\n",
    "label_list = sorted({t for _x, tags in examples for t in tags})\n",
    "label2id = {l: i for i, l in enumerate(label_list)}; id2label = {i: l for l, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3  Dataset & tokenisation"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hf(example):\n    \ttoks, tags = example\n    \treturn {'tokens': toks, 'ner_tags': [label2id[t] for t in tags]}\n",
    "ds = datasets.Dataset.from_list([to_hf(e) for e in examples])\n",
    "ds = ds.shuffle(seed=42).train_test_split(test_size=0.2)\n",
    "train_ds, eval_ds = ds['train'], ds['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n    \tenc = tokenizer(batch['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)\n    \tlabels = []\n    \tfor i, word_ids in enumerate(enc.word_ids(batch_index=None)):\n    \t\tword_ids = enc.word_ids(batch_index=i)\n    \t\tlabel_ids, prev = [], None\n    \t\tfor wid in word_ids:\n    \t\t\tif wid is None: label_ids.append(-100)\n    \t\t\telif wid != prev:\n    \t\t\t\tlabel_ids.append(batch['ner_tags'][i][wid]); prev = wid\n    \t\t\telse: label_ids.append(batch['ner_tags'][i][wid])\n    \t\tlabels.append(label_ids)\n    \tenc['labels'] = labels\n    \treturn enc\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=eval_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4  Fine-tune"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "args = TrainingArguments(\n        output_dir='models/ner-xlmr',\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        evaluation_strategy='epoch',\n        save_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        fp16=torch.cuda.is_available(),\n    )\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "seqeval = evaluate.load('seqeval')\n",
    "def metrics(p):\n        logits, labels = p\n        preds = logits.argmax(-1)\n        true_preds, true_labels = [], []\n        for pred, lab in zip(preds, labels):\n            tp, tl = [], []\n            for p_i, l_i in zip(pred, lab):\n                if l_i != -100:\n                    tp.append(id2label[p_i]); tl.append(id2label[l_i])\n            true_preds.append(tp); true_labels.append(tl)\n        return seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=data_collator, compute_metrics=metrics)\n",
    "# Uncomment to train\n    # trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["After training, the best checkpoint resides in `models/ner-xlmr/`. Load with:\n\n```python\nfrom transformers import AutoModelForTokenClassification\nmodel = AutoModelForTokenClassification.from_pretrained('models/ner-xlmr')\n```"]
  }
 ]
}
